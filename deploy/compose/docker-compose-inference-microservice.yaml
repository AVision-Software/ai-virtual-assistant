services:
  nemollm-inference:
    container_name: nemollm-inference-microservice
    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:latest
    volumes:
    # Use current path for model directory if nothing is specified
    - ${MODEL_DIRECTORY:-.}:/opt/nim/.cache
    user: "${USERID:-1000:1000}"
    ports:
    - "8000:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
    shm_size: 16gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              # device_ids: ['${LLM_MS_GPU_ID:-0,1,2,3}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 10s
      timeout: 20s
      retries: 100

networks:
  default:
    name: nvidia-rag
